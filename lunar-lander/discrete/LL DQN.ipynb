{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-learning solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy policy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) update (neural network loss function):  \n",
    "![network_loss](images/dqn_loss.png)  \n",
    "with  \n",
    "\n",
    "![return](images/return_definition.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters:**  \n",
    "*   $\\alpha$ (alpha) is the learning rate (0<$\\Large \\alpha$<=1) \n",
    "*   $\\gamma$ (gamma) is the discount factor ($0 \\leq \\gamma \\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
    "*   $\\epsilon$ (epsilon) is the probability of random action. In our case we start from full exploration (epsilon=1.0) and decrease the value to epsilon_min (=0.01) as episodes pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converges after ~600 iterations, with lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 9]\n",
    "from lunar_lander_dqn import DQN, create_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env()\n",
    "agent = DQN(env, alpha=0.001, gamma=.99, epsilon=1.0, epsilon_min=.01, epsilon_max=1.0, epsilon_decay=0.996, batch_size=64)\n",
    "reward = agent.train(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.load(r'dqn_reward_600.npy')\n",
    "freq = 1\n",
    "plt.plot(np.arange(1, len(reward)+1, freq), reward[::freq], label='final reward')\n",
    "window = 30\n",
    "plt.plot(pd.DataFrame(reward).rolling(window).mean(), label='moving average on final reward')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Excecute evaluation in the terminal (TF multi-threading issues with IPython kernel).\n",
    "python evaluate_dqn.py checkpoints\\dqn_e100.h5 1  \n",
    "python evaluate_dqn.py checkpoints\\dqn_e300.h5 1  \n",
    "python evaluate_dqn.py checkpoints\\dqn_e600.h5 5  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2] *",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
