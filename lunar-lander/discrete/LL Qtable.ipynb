{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Gym. LunarLander-v2 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. \n",
    "Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
    "If lander moves away from landing pad it loses reward back. \n",
    "Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. \n",
    "Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. \n",
    "Landing outside landing pad is possible. \n",
    "Fuel is infinite, so an agent can learn to fly and then land on its first attempt. \n",
    "Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**  \n",
    "To guide the lander to the center of the landing pad and zero speed, maintaining correct orientation (both legs pointing down)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State vector (s):**  \n",
    "* s[0] - x coordinate  \n",
    "* s[1] - y coordinate  \n",
    "* s[2] - x speed  \n",
    "* s[3] - y speed  \n",
    "* s[4] - angle  \n",
    "* s[5] - angular speed  \n",
    "* s[6] - if first leg has contact  \n",
    "* s[7] - if second leg has contact  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Possible actions:**   \n",
    "- 0 - Do nothing  \n",
    "- 1 - Fire left engine  \n",
    "- 2 - Fire main engine  \n",
    "- 3 - Fire right engine  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rewards:**\n",
    "*   +100..140 for landing on the pad\n",
    "*   +100 for successful landing\n",
    "*   +10 for each led ground contact\n",
    "*   -100 for crashing into the ground\n",
    "*   -0.3 for each firing of the main engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python lunar_lander_keyboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic solution (fuzzy controller)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python lunar_lander_heuristic.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\epsilon$-greedy policy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD(0) update:  \n",
    "![q-value equation](images/math.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters:**  \n",
    "*   $\\alpha$ (alpha) is the learning rate (0<$\\Large \\alpha$<=1) \n",
    "*   $\\gamma$ (gamma) is the discount factor ($0 \\leq \\gamma \\leq 1$) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
    "*   $\\epsilon$ (epsilon) is the probability of random action. In our case we start from full exploration (epsilon=1.0) and decrease the value to epsilon_min (=0.01) as episodes pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [15, 9]\n",
    "from lunar_lander_qtable_td import QTable, create_env\n",
    "env = create_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent on N episodes\n",
    "agent = QTable(env, alpha=0.1, gamma=.99, epsilon=1.0, epsilon_min=.01, epsilon_max=1.0, epsilon_decay=0.996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = agent.train(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = np.load(r'qtbl_td_reward_20000.npy')\n",
    "window = 500\n",
    "freq = 20\n",
    "avg_reward = pd.DataFrame(reward).rolling(window).mean()\n",
    "plt.plot(np.arange(1, len(reward)+1, freq), reward[::freq], label='final reward')\n",
    "plt.plot(avg_reward, label='moving average on final reward')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('reward')\n",
    "plt.title('Final reward over trainig episodes')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-table visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load q-tables of agents with different performance\n",
    "agent_1k = QTable(env)\n",
    "# agent_1k.load_qtable(r'checkpoints\\qtbl_td_e1000.npy')\n",
    "agent_1k.load_qtable(r'checkpoints\\qtbl_td_e1000.npy')\n",
    "agent_20k = QTable(env)\n",
    "# agent_20k.load_qtable(r'checkpoints\\qtbl_td_e20000.npy')\n",
    "agent_20k.load_qtable(r'checkpoints\\qtbl_td_e20000.npy')\n",
    "qtbl_1k = agent_1k.qtbl_2d\n",
    "qtbl_20k = agent_20k.qtbl_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,30)) \n",
    "actions = ['idle', 'left engine', 'main engine', 'right engine']\n",
    "start_idx = 18000\n",
    "n_samples = 300\n",
    "ax[0].set_title('for 1000 episodes')\n",
    "ax[1].set_title('for 20000 episodes')\n",
    "x_1k = sns.heatmap(qtbl_1k[start_idx:start_idx+n_samples], xticklabels=actions, ax=ax[0])\n",
    "x_20k = sns.heatmap(qtbl_20k[start_idx:start_idx+n_samples], xticklabels=actions, ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon decay function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 20000\n",
    "episodes = np.arange(total_episodes)\n",
    "# epsilon = np.clip(1.0 - np.log10((episodes + 1) / (total_episodes * 0.1)), 0.1, 1.0)\n",
    "epsilon = agent.decay_function(episodes, total_episodes)\n",
    "plt.rcParams['figure.figsize'] = [15, 9]\n",
    "plt.plot(episodes, epsilon, label='epsilon decay')\n",
    "plt.plot(episodes, (avg_reward - np.min(avg_reward)) / (np.max(avg_reward) - np.min(avg_reward)), label='reward')\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('epsilon')\n",
    "plt.legend()\n",
    "plt.title('Decaying epsilon and reward over training episodes')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent performance at the beginning (1000 episodes)\n",
    "<!-- ![before_training](images/training/before_training.gif) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_qtable.py \"checkpoints\\qtbl_td_e1000.npy\" 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent performance after 5000 episodes\n",
    "<!-- ![mid_training](images/training/mid_training.gif) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_qtable.py \"checkpoints\\qtbl_td_e5000.npy\" 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent performance after 20000 episodes\n",
    "<!-- ![after_training](images/training/after_training.gif) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_qtable.py \"checkpoints\\qtbl_td_e20000.npy\" 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2] *",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
